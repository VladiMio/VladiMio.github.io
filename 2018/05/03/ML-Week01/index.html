<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="VladiMio,jasper1992ws@hotmail.com"><title>机器学习笔记 01 · 简单易懂的现代魔法</title><meta name="description" content="“A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measure"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title=""><a href="/">简单易懂的现代魔法</a></h3><div class="description"><p>万国の労働者よ、団結せよ!</p></div></div></div><ul class="social-links"><li><a href="http://weibo.com/u/1596079301"><i class="fa fa-weibo"></i></a></li><li><a href="http://github.com/VladiMio"><i class="fa fa-github"></i></a></li></ul></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">首页</a></li><li><a href="/archives">归档</a></li></div><div class="information"><div class="back_btn"><li><a onclick="window.history.go(-1)" class="fa fa-chevron-left"> </a></li></div><div class="avatar"><img src="/images/favicon.png"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>机器学习笔记 01</a></h3></div><div class="post-content"><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<p><em>“A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.”</em></p>
<p>机器学习算法可以大致分为两类：Supervised Learning（监督学习）和Unsupervised Learning（无监督学习）。 * Supervised Learnin：从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。例如 Regression Problem (Predict real-valued output) * Unsupervised Learning：与监督学习相比，无监督学习的训练集没有人为标注的结果。</p>
<p>机器学习问题中常用的符号定义如下：</p>
<p><strong>m</strong> = Number of training examples</p>
<p><strong>x</strong> = input values/ features</p>
<p><strong>y</strong> = output values/ target variable</p>
<p><strong>(x<sup>(i)</sup>, y<sup>(i)</sup>)</strong> = the No.i training example</p>
<h3 id="hypothesis-假设函数">Hypothesis (假设函数)</h3>
<p>一个机器学习算法运行的方式如下图所示</p>
<p><img src="http://ogptt11t8.bkt.clouddn.com/MLWeek01-001.png"></p>
<p>其中函数</p>
<p><span class="math display">\[ h_\theta(x) = \theta_0 + \theta_1x \]</span></p>
<p>称为 <strong>hypothesis (假设函数)</strong></p>
<p><em>&amp;#42以上函数h(x)仅针对线性情况</em></p>
<h3 id="cost-function-代价函数">Cost Function (代价函数)</h3>
<p>针对一个线性回归问题的假设函数</p>
<p><span class="math display">\[ h_\theta(x) = \theta_0 + \theta_1x \]</span></p>
<p>如何选择 <span class="math inline">\(\theta_0\)</span> 和 <span class="math inline">\(\theta_1\)</span> 可以使 <span class="math inline">\(h_\theta(x)\)</span> 更好地拟合训练数据集呢？一般认为，当通过 <span class="math inline">\(h_\theta(x)\)</span> 得出的预测值与训练数据集中的样本值方差最小时，<span class="math inline">\(\theta_0\)</span> 和 <span class="math inline">\(\theta_1\)</span> 取得最优值，即函数</p>
<p><span class="math display">\[ J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x_i)-y_i)^2 \]</span></p>
<p>取得最小值时，<span class="math inline">\(\theta_0\)</span> 和 <span class="math inline">\(\theta_1\)</span> 取得最优值。<span class="math inline">\(J(\theta_0,\theta_1)\)</span> 被称作 <strong>Cost Function (代价函数)</strong></p>
<h3 id="gradient-descent-梯度下降">Gradient Descent (梯度下降)</h3>
<p>为了使代价函数 <span class="math inline">\(J(\theta_0,\theta_1)\)</span> 取得最小值，我们采用一种叫做 <strong>Gradient Descent (梯度下降)</strong> 的算法——确切地来说，梯度下降算法只能帮助我们到达函数的局部极小值，因此在代价函数较为复杂的情况下可能会出现问题。</p>
<p>首先在代价函数 <span class="math inline">\(J(\theta_0,\theta_1)\)</span> 上任意取得一点， 然后向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索： 假设有函数 <span class="math inline">\(F(x)\)</span> 在 <span class="math inline">\(a\)</span> 点处存在且可微，那么如果 <span class="math inline">\(b = a - \gamma\nabla F(a)\)</span> 对于任意小的 <span class="math inline">\(\gamma &gt; 0\)</span>成立，则有 <span class="math inline">\(F(a) \geq F(b)\)</span>。 因此我们只要对 <span class="math inline">\(J(\theta_0,\theta_1)\)</span> 上任意一点 <span class="math inline">\((\theta_0,\theta_1)\)</span> 进行如下迭代：</p>
<p><span class="math display">\[ \theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1) \]</span></p>
<p>可以期望 <span class="math inline">\(\theta_j\)</span> 最终收敛于局部极小值。其中，迭代步长 <span class="math inline">\(\alpha\)</span> 被称为 <strong>Learning Rate (学习速率)</strong> ，并在每一次迭代中都可以改变。</p>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2018-05-03</span><i class="fa fa-comment-o"></i><a href="/2018/05/03/ML-Week01/#comments">评论</a><i class="fa fa-tag"></i><a href="/categories/Machine-learning/" title="Machine-learning" class="tag">Machine-learning </a><a href="/tags/学习笔记/" title="学习笔记" class="tag">学习笔记 </a></div></div></div></div><div class="share"><div class="evernote"><a href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank" class="fa fa-bookmark"></a></div><div class="weibo"><a href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));" class="fa fa-weibo"></a></div><div class="twitter"><a href="http://twitter.com/home?status=,http://VladiMio.github.io/2018/05/03/ML-Week01/,简单易懂的现代魔法,机器学习笔记 01,;" class="fa fa-twitter"></a></div></div><div class="pagination"><ul class="clearfix"><li class="next pagbuttons"><a role="navigation" href="/2018/03/05/OpenGL_VAO&amp;VBO/" title="OpenGL - VAO &amp; VBO" class="btn">下一篇</a></li></ul></div><a id="comments"></a><div id="disqus_thread"></div><script>var disqus_shortname = 'vladimio-me';
var disqus_identifier = '2018/05/03/ML-Week01/';
var disqus_title = '机器学习笔记 01';
var disqus_url = 'http://VladiMio.github.io/2018/05/03/ML-Week01/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//vladimio-me.disqus.com/count.js" async></script></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>